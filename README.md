# turing-data-engineering-challenge
A repository for the codes and result of Turing Inc. Data Engineering Challenge. 

## Description
This challenge involves obtaining large amounts of data from web sources and processing them in a distributed manner under given constraints. Data from 100,000 public Github repositories were processed to determine number of lines, external libraries used, average nesting factor, percentage of code duplication, average number of parameters and average number of variables of each repository.

## Getting Started
A distributed system of 3 AWS EC2 Linux micro instances was used to distribute the data from the repositories. Two instances processed data from 33,000 repositories, and 1 processed data from 34,000 repositories, making it 100,000 repositories processed.
The libraries required for this project is in the requirements.txt.  

Run ‘pip install –r  requirements.txt’ to quickly install the libraries, after cloning this repository.


The major part of the code lives in turing.py. getpass.py is used to automatically authenticate Github while collecting data needed for the process by cloning repositories.


## Running the Code and Deploying
Some of the tools used in executing the project include, PuTTY and  FileZilla. Putty is used in connected to the remote AWS EC2 from a local host, and FileZilla makes transfer of files between a local host and the remote hosts seamless. PuTTY can be downloaded from https://www.putty.org/ and FileZilla can be downloaded from https://filezilla-project.org/download.php 


Set the environment variables GIT_USERNAME and GIT_PASSWORD. The GIT_USERNAME and GIT_PASSWORD should be your Github username and password respectively. These would use by getpass for Github authentication.  

Run ‘python turing.py’ to execute the code. 


 ## Result
Three JSON file was generated by the 3 distributed computing system. A Python script resultmerge.py merges the three results to get the final result, results.json 

# USING AWS REDSHIFT

A distributed processing of the data was attempted with AWS Redshift. Amazon Redshift is a fully managed, petabyte-scale data warehouse service in the cloud.  The data was converted into a semi-structured data in form of a JSON file saved in AWS S3. It was then loaded into Redshift from S3 and processed.


The code for this lives in turing_redshift.py.
