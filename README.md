# turing-data-engineering-challenge
A repository for the codes and result of Turing Inc. Data Engineering Challenge. 

# Description
This challenge involves obtaining large amounts of data from web sources and processing them in a distributed manner under given constraints. Data from 100,000 public Github repositories were processed to determine number of lines, external libraries used, average nesting factor, percentage of code duplication, average number of parameters and average number of variables of each repository.

# Getting Started
A distributed system over 22 AWS EC2 Linux micro instances was used to distribute the data from the repositories. 

An instances processed data from 4,000 repositories at a time, and 100,000 repositories processed were processed.
The libraries required for this project is in the `requirements.txt`. 

To get this repo running:

* Install Python 3.  You can find instructions [here](https://wiki.python.org/moin/BeginnersGuide/Download).
* Create a [virtual environment](https://docs.python.org/3/library/venv.html).
* Clone this repo with `git clone git@github.com:frankburg//turing-data-engineering-challenge.git`
* Get into the folder with `cd /turing-data-engineering-challenge`
* Install the requirements with `pip install -r requirements.txt`



The major part of the code lives in `turing.py`.


# Running the Code and Deploying
Some of the tools used in executing the project include, PuTTY and  FileZilla. 
* Putty is used in connected to the remote AWS EC2 from a local host, and FileZilla makes transfer of files between a local host and the remote hosts seamless. 
* PuTTY can be downloaded from [here](https://www.putty.org/) 
* FileZilla can be downloaded from [here](https://filezilla-project.org/download.php) 


* Run `python turing.py` to execute the code. 


 # Result
Three JSON file was generated by the 3 distributed computing system. A Python script `result_merge.py` merges the three results to get the final result, results.json 

# USING AWS REDSHIFT

A distributed processing of the data was attempted with AWS Redshift. Amazon Redshift is a fully managed, petabyte-scale data warehouse service in the cloud.  The data was converted into a semi-structured data in form of a JSON file saved in AWS S3. It was then loaded into Redshift from S3 and processed.


The code for this lives in `turing_redshift.py`.

## WorkFlow
* The `url_list.csv` containing the urls of Github repositories was uploaded to an EC2 instance.
* Each repository is clone to an EC2 instance and uploaded imediately and seamlessly to `Amazon S3`. 
* The data is transform to a JSON
* Loaded to a `Redshift cluster` and processed.

